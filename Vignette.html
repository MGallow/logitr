<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Matt Galloway" />

<meta name="date" content="2017-05-25" />

<title>logitr</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">logitr</h1>
<h4 class="author"><em>Matt Galloway</em></h4>
<h4 class="date"><em>2017-05-25</em></h4>


<div id="TOC">
<ul>
<li><a href="#overview">Overview</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#logistic-regression">Logistic Regression</a><ul>
<li><a href="#maximum-likelihood-estimation">Maximum Likelihood Estimation</a></li>
<li><a href="#ridge-logistic-regression">Ridge Logistic Regression</a></li>
<li><a href="#bridge-logistic-regression">Bridge Logistic Regression</a></li>
</ul></li>
<li><a href="#iterative-re-weighted-least-squares-irls">Iterative Re-Weighted Least Squares (IRLS)</a></li>
<li><a href="#majorize-minimization-mm">Majorize-Minimization (MM)</a></li>
<li><a href="#solve-for-qbeta-betak">Solve for <span class="math inline">\(Q(\beta | \beta^{(k)})\)</span></a></li>
<li><a href="#usage">Usage</a></li>
<li><a href="#example">Example</a></li>
<li><a href="#computer-specs">Computer Specs:</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<div id="overview" class="section level2">
<h2>Overview</h2>
<p><code>logitr</code> is an R package for linear and logistic regression with optional ridge and bridge regularization penalties. In this report we will provide a brief theoretical justification for using logistic regression as well as examples and simulation results for this package. A list of functions contained in the package (available to user) can be found below:</p>
<ul>
<li><code>logisticr()</code> computes the coefficient estimates for logistic regression (ridge and bridge regularization, optional)</li>
<li><code>linearr()</code> computes the linear regression coefficient estimates (ridge regularization and weights, optional)</li>
<li><code>predict_logisticr()</code> generates predictions and loss metrics for logistic regression</li>
<li><code>predict_linearr()</code> generates predictions and loss metrics for linear regression</li>
</ul>
</div>
<div id="installation" class="section level2">
<h2>Installation</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># The easiest way to install is from the development version from GitHub:</span>
<span class="co"># install.packages(&quot;devtools&quot;)</span>
devtools::<span class="kw">install_github</span>(<span class="st">&quot;MGallow/logitr&quot;</span>)</code></pre></div>
<p>If there are any issues/bugs, please let me know: <a href="https://github.com/MGallow/logitr/issues/" class="uri">https://github.com/MGallow/logitr/issues/</a>.</p>

</div>
<div id="logistic-regression" class="section level2">
<h2>Logistic Regression</h2>
<p>Logistic regression is used in a variety of fields and subjects when the goal of the analysis is to predict the likelihood or probability of a particular event occurring. We typically denote instances where an event occurs as a “success” and a “failure” when the event does not occur. These names, however, are often catered to the particular application of interest. For instance, we may opt to use “spam” and “not spam” in email spam detection and “cat” and “dog” in object recognition. <code>logitr</code> was built with particular applications like these in mind. In order to better understand the code contained in the package, we will provide a brief mathematical summary in the following paragraphs. </p>
<p>Let us consider a two-class classification problem. We denote the response <span class="math inline">\(y_{i}\)</span> for the <span class="math inline">\(i\)</span>th observation being contained in the set <span class="math inline">\(\left \{ 0, 1 \right \}\)</span>. In this set up (without loss of generality), “1” is the success and “0” is the failure. Furthermore, we denote our observed variables <span class="math inline">\(x_{1}, ..., x_{p}\)</span> as a column vector <span class="math inline">\(x_{i} = (1, x_{i1}, x_{i2}, ..., x_{ip})^{T}\)</span>. Given the information in <span class="math inline">\(x_{i}\)</span>, it is the goal of logistic regression to predict the probability that <span class="math inline">\(y_{i} = 1\)</span>. That is, the conditional probability <span class="math inline">\(P(Y_{1} = 1 | X_{i} = x_{i}) = 1 - P(Y_{i} = 0 | X_{i} = x_{i})\)</span>.</p>
<p>Once this probability is calculated, we can define a <strong>decision rule</strong> (denoted by <span class="math inline">\(\phi\)</span>) that classifies observation <span class="math inline">\(i\)</span> as either “0” or “1”. Intuititvely, we might say that if the probability of observation <span class="math inline">\(i\)</span> being in class “1” is greater than the probability observation <span class="math inline">\(i\)</span> being in class “0”, then we should classify observation <span class="math inline">\(i\)</span> as class “1” (and “0” otherwise). Indeed, this is the optimal decision rule given to us by the <strong>Baye’s Rules</strong>. The decision rule is the following:</p>
<p><span class="math display">\[ \phi(x_{i}) = \begin{cases}
1 &amp; \text{ if } P(Y_{i} = 1 | X_{i} = x_{i}) \geq P(Y_{i} = 0 | X_{i} = x_{i}) \\ 
0 &amp; \text{ otherwise }
\end{cases} \]</span></p>
<p>This decision rule can be summed up as choosing the class that has the greatest probability conditional on the observed information <span class="math inline">\(x_{i}\)</span>. An equivalent decision rule uses what is called the <strong>log-odds ratio</strong>:</p>
<p><span class="math display">\[ \phi(x_{i}) = \begin{cases}
1 &amp; \text{ if } \log\left (\frac{P(Y_{i} = 1 | X_{i} = x_{i})}{P(Y_{i} = 0 | X_{i} = x_{i})} \right ) \geq 0 \\ 
0 &amp; \text{ otherwise }
\end{cases} \]</span></p>
<p>where the <span class="math inline">\(\log(\cdot)\)</span> term is the log-odds ratio.</p>
<p>This formulation is convenient because in logistic regression we make the critical assumption that this log-odds ratio is linear in form. That is, we can estimate the log-odds ration using some vector of coefficients <span class="math inline">\(\beta = (\beta_{0}, \beta_{1}, ..., \beta_{p})^{T}\)</span>:</p>
<p><span class="math display">\[ \log \left (\frac{P(Y_{i} = 1 | X_{i} = x_{i})}{P(Y_{i} = 0 | X_{i} = x_{i})} \right ) \approx x_{i}^{T}\beta \]</span></p>
<p>This implies the following:</p>
<span class="math display">\[\begin{align}
\log &amp;\left (\frac{P(Y_{i} = 1 | X_{i} = x_{i})}{P(Y_{i} = 0 | X_{i} = x_{i})} \right ) = \log \left (\frac{P(Y_{i} = 1 | X_{i} = x_{i})}{1 - P(Y_{i} = 1 | X_{i} = x_{i})} \right ) \approx x_{i}^{T}\beta \\
  &amp;\Rightarrow \frac{P(Y_{i} = 1 | X_{i} = x_{i})}{1 - P(Y_{i} = 1 | X_{i} = x_{i})} \approx \exp(x_{i}^{T}\beta) \\
  &amp;\Rightarrow P(Y_{i} = 1 | X_{i} = x_{i}) \approx \frac{\exp(x_{i}^{T}\beta)}{1 + \exp(x_{i}^{T}\beta)} \equiv p(x_{i}) \mbox{ (say)}
\end{align}\]</span>
<p>We see that the conditional probability <span class="math inline">\(P(Y_{i} = 1 | X_{i} = x_{i})\)</span> can be approximated by the function <span class="math inline">\(p(x_{i})\)</span> – we call this function the <strong>logit</strong> function (hence the name of our package!). We will show the method for solving for the optimal vector of coefficients (<span class="math inline">\(\hat{\beta}\)</span>) in the next section.</p>
<div id="maximum-likelihood-estimation" class="section level3">
<h3>Maximum Likelihood Estimation</h3>
<p>The classical statistical method of optimization for linear and generalized linear models is called <strong>maximum likelihood estimation</strong>. In short, we want to maximize the probability of our observed data. This requires defining a probability distribution. Recall that our formulation assumes <span class="math inline">\(Y_{i}\)</span> is some random variable that takes values in the set <span class="math inline">\(\left \{ 0, 1 \right \}\)</span>. By definition, this random variable follows a Bernoulli distribution:</p>
<p><span class="math display">\[ P(Y_{i} = y_{i} | X_{i} = x_{i}) = \begin{cases}
p(x_{i}) &amp; \text{ if } y_{i} = 1\\ 
1 - p(x_{i}) &amp; \text{ if } y_{i} = 0
\end{cases} \]</span></p>
<p>Equivalently, we can write this as:</p>
<p><span class="math display">\[ P(Y_{i} = y_{i} | X_{i} = x_{i}) = p(x_{i})^{y_{i}}(1 - p(x_{i}))^{1 - y_{i}} \]</span></p>
<p>Having defined a probability distribution, we can now construct our (log) likelihood function (assume an iid setting):</p>
<span class="math display">\[\begin{align}
l(\beta) = \log L(\beta) &amp;=  \log \left [ \prod_{i = 1}^{n}p(x_{i})^{y_{i}}(1 - p(x_{i})^{1 - y_{i}}) \right ] \\
  &amp;= \log \left [\prod_{i = 1}^{n} \left (\frac{\exp(x_{i}^{T}\beta)}{1 + \exp(x_{i}^{T}\beta)} \right)^{y_{i}} \left (1 - \frac{\exp(x_{i}^{T}\beta)}{1 + \exp(x_{i}^{T}\beta)} \right)^{1 - y_{i}} \right] \\
  &amp;= \log \left [\prod_{i = 1}^{n} \left (\frac{\exp(x_{i}^{T}\beta)}{1 + \exp(x_{i}^{T}\beta)} \right)^{y_{i}} \left (\frac{1}{1 + \exp(x_{i}^{T}\beta)} \right)^{1 - y_{i}} \right] \\
&amp;= \log \left [\prod_{i = 1}^{n} (\exp(x_{i}^{T}\beta))^{y_{i}}(1 + \exp(x_{i}^{T}\beta))^{-1} \right] \\
&amp;= \sum_{i = 1}^{n}y_{i}(x_{i}^{T}\beta) - \sum_{i = 1}^{n}\log(1 + \exp(x_{i}^{T}\beta))
\end{align}\]</span>
<p>Maximum likelihood estimation involves find the <span class="math inline">\(\beta\)</span> vector that maximizes the log-likelihood function – or equivalently, minimizes the negative log-likelihood:</p>
<p><span class="math display">\[ \hat{\beta} = \arg\min_{\beta} -l(\beta)  \]</span></p>
<p>In addition, we can add regularization terms if we so choose:</p>
</div>
<div id="ridge-logistic-regression" class="section level3">
<h3>Ridge Logistic Regression</h3>
<p><span class="math display">\[ \hat{\beta} = \arg\min_{\beta} -l(\beta) + \frac{\lambda}{2}\sum_{j = 1}^{p}\beta_{j}^{2}  \]</span></p>
</div>
<div id="bridge-logistic-regression" class="section level3">
<h3>Bridge Logistic Regression</h3>
<p><span class="math display">\[ \hat{\beta} = \arg\min_{\beta} -l(\beta) + \lambda\frac{1}{\alpha}\sum_{j = 1}^{p}\left|\beta_{j} \right|^{\alpha}  \]</span></p>
<p>where <span class="math inline">\(\lambda \geq 0\)</span> and <span class="math inline">\(\alpha \in \left(1, 2 \right)\)</span>. Note that we do not penalize the intercept in either method. Algorithms for computing <span class="math inline">\(\hat{\beta}\)</span> will be the subject of our next section.</p>

<p><strong>Note:</strong></p>
<span class="math display">\[\begin{align*}
\nabla l(\beta) &amp;= \sum_{i = 1}^{n}y_{i}x_{i} - \sum_{i = 1}^{n}\left (\frac{x_{i}\exp(x_{i}^{T}\beta)}{1 + \exp(x_{i}^{T}\beta)} \right) = \sum_{i = 1}^{n}y_{i}x_{i} - \sum_{i = 1}^{n}x_{i}p(x_{i}) \\
  &amp; \\
\nabla^{2} l(\beta) &amp;= -\sum_{i = 1}^{n}\left (\frac{x_{i}x_{i}^{T}\exp(x_{i}^{T}\beta)(1 + \exp(x_{i}^{T}\beta)) - x_{i}x_{i}^{T}\exp(x_{i}^{T}\exp(x_{i}^{T}))}{(1 + \exp(x_{i}^{T}\beta))^{2}} \right) \\
  &amp;= -\sum_{i = 1}^{T}\left (\frac{x_{i}x_{i}^{T}\exp(x_{i}^{T}\beta)}{(1 + \exp(x_{i}^{n}\beta))^{2}} \right) = -\sum_{i = 1}^{n}x_{i}x_{i}^{T}p(x_{i})(1 - p(x_{i})) = \sum_{i = 1}^{n}x_{i}x_{i}^{T}w(x_{i}) \mbox{ (say)}
\end{align*}\]</span>

</div>
</div>
<div id="iterative-re-weighted-least-squares-irls" class="section level2">
<h2>Iterative Re-Weighted Least Squares (IRLS)</h2>
<p>The first algorithm we propose to compute the optimal <span class="math inline">\(\beta\)</span> is called iterative re-weighted least squares. The reasoning behind the name will be clear once the method is fully derived. For convenience, in this section we will assume that there is no intercept.</p>
<p>According to Taylor’s Theorem (Newton-Raphson),</p>
<p><span class="math display">\[ -l(\beta) = -l(\beta^{(k)}) + \frac{1}{2}(\beta - \beta^{(k)})^{T}(\nabla^{2} -l(\beta^{(k)}))(\beta - \beta^{(k)}) + (\nabla - l(\beta^{(k)}))^{T}(\beta - \beta^{(k)}) \]</span></p>
<p>where <span class="math inline">\(\beta^{(k)}\)</span> is some estimator for the true <span class="math inline">\(\beta\)</span>. In this case <span class="math inline">\(\beta^{(k)}\)</span> is the <span class="math inline">\(\beta\)</span> estimate for the <span class="math inline">\(k\)</span>th iteration. Dropping constants and adding our regularization term (only ridge penalty – bridge will be computed using MM algorithm), we see the following:</p>
<span class="math display">\[\begin{align*}
\beta^{(k + 1)} &amp;= \arg\min_{\beta} \left \{ \frac{1}{2}(\beta - \beta^{(k)})^{T}(\nabla^{2} -l(\beta))(\beta - \beta^{(k)}) + (\nabla - l(\beta))^{T}(\beta - \beta^{(k)}) + \frac{\lambda}{2}\sum_{j = 1}^{p}\beta_{j}^{2} \right\} \\
  &amp;= \arg\min_{\beta} \left \{ \frac{1}{2}(\beta - \beta^{(k)})^{T}(\sum_{i = 1}^{n}x_{i}x_{i}^{T}w_{i})(\beta - \beta^{(k)}) -\sum_{i = 1}^{n}x_{i}^{T}(y_{i} - p_{i}^{(k)})^{T}(\beta - \beta^{(k)}) + \frac{\lambda}{2}\sum_{j = 1}^{p}\beta_{j}^{2} \right \} \\
  &amp;= \arg\min_{\beta} \left \{ \frac{1}{2}\sum_{i = 1}^{n}w_{i}^{(k)}(x_{i}^{T}\beta - x_{i}^{T}\beta^{(k)})^{T}(x_{i}^{T}\beta - x_{i}^{T}\beta^{(k)}) - \sum_{i = 1}^{n}w_{i}^{(k)}(x_{i}^{T}\beta - x_{i}^{T}\beta^{(k)})\frac{y_{i} - p_{i}^{(k)}}{w_{i}^{(k)}} + \frac{\lambda}{2}\sum_{j = 1}^{p}\beta_{j}^{2} \right \} \\
  &amp;= \arg\min_{\beta} \left \{\sum_{i = 1}^{n}w_{i}^{(k)}\left (\frac{y_{i} - p_{i}^{(k)}}{w_{i}^{(k)}} + x_{i}^{T}\beta^{(k)} - x_{i}^{T}\beta \right)^{2} + \frac{\lambda}{2}\sum_{j = 1}^{p}\beta_{j}^{2} \right \} \\
  &amp;= \arg\min_{\beta} \left \{\sum_{i = 1}^{n}w_{i}^{(k)}(z_{i}^{(k)} - x_{i}^{T}\beta)^{2} + \frac{\lambda}{2}\sum_{j = 1}^{p}\beta_{j}^{2} \right \} \\
  &amp;= \arg\min_{\beta} \left \{\frac{1}{2}(z^{(k)} - X\beta)^{T}W^{(k)}(z^{(k)} - X\beta) + \frac{\lambda}{2}\beta^{T}\beta \right \}
\end{align*}\]</span>
<p>where <span class="math inline">\(z^{(k)}\)</span> is the column vector with <span class="math inline">\(i\)</span>th entry <span class="math inline">\(z_{i}^{(k)} = \frac{y_{i} - p_{i}^{(k)}}{w_{i}^{(k)}} + x_{i}^{T}\beta^{(k)}\)</span> for the <span class="math inline">\(k\)</span>th iteration. Further, <span class="math inline">\(W^{(k)}\)</span> is a diagonal matrix with <span class="math inline">\(i\)</span>th diagonal element <span class="math inline">\(w_{i}^{(k)} = p(x_{i})(1 - p(x_{i}))\)</span> and <span class="math inline">\(X\)</span> is an <span class="math inline">\(n \times p\)</span> matrix containing all <span class="math inline">\(n\)</span> (sample size) observations.</p>
<p>We can see that this is a weighted-least squares problem! The iterative aspect comes from the fact that we will be solving this minimization problem and looping it for <span class="math inline">\(k = 0, 1, ..., K\)</span> until convergence. Solving for the gradient and setting it equal to zero implies that,</p>
<p><span class="math display">\[ \beta^{(k + 1)} = (X^{T}W^{(k)}X + \lambda I_{p})^{-1}X^{T}W^{(k)}z^{(k)} \]</span></p>
<p>If <span class="math inline">\(p\)</span> is particularly large (say <span class="math inline">\(p &gt;&gt; n\)</span>) then we can solve an alternative formulation:</p>
<p><span class="math display">\[\beta^{(k + 1)} = X^{T}\sqrt{W^{(k)}}(\sqrt{W^{(k)}}XX^{T}\sqrt{W^{(k)}} + \lambda I_{p})^{-1}\sqrt{W^{(k)}}z^{(k)} \]</span></p>
<p>This formulation makes use of what’s called the “kernel trick”. Note that in the latter formulation, we are only inverting a <span class="math inline">\(p \times p\)</span> matrix (as opposed to an <span class="math inline">\(n \times n\)</span> matrix). This allows us to save significant computation time when <span class="math inline">\(p\)</span> is particularly large. The <code>logitr</code> package will solve this formulation by default when <span class="math inline">\(p &gt; n\)</span>.</p>
<p>Putting everything together, we have the IRLS algorithm:</p>

<p><strong>Algorithm</strong>:</p>
<ul>
<li>Intialize <span class="math inline">\(\beta^{(0)}, W^{(0)}\)</span>.</li>
<li>For <span class="math inline">\(k = 0, ..., K\)</span> until convergence:
<ul>
<li><span class="math inline">\(p^{(k + 1)} = logit(X\beta^{(k)})\)</span></li>
<li><span class="math inline">\(W^{(k + 1)} = p^{(k + 1)}(1 - p^{(k + 1)})\)</span></li>
<li><span class="math inline">\(z^{(k + 1)} = (y - p^{(k + 1)})/W^{(k + 1)} + X\beta^{(k)}\)</span></li>
<li><span class="math inline">\(\beta^{(k + 1)} = (X^{T}W^{(k)}X + \lambda I_{p})^{-1}X^{T}W^{(k)}z^{(k)}\)</span></li>
</ul></li>
</ul>

<p>Below you can find two snippets of code from the <code>logitr</code> package. The first snippet solves the weighted least squares problem. Note that this portion uses SVD decomposition which will not be discussed in this report. The second snippet is the code for IRLS.</p>

<p><strong>Weighted least squares code snippet</strong>:</p>
<p>Note this is not the actual code. The real code is written in c++.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#calculate the coefficient estimates for weighted least squares</span>
linearr =<span class="st"> </span>function(X, y, <span class="dt">lam =</span> <span class="dv">0</span>, <span class="dt">weights =</span> <span class="ot">NULL</span>, <span class="dt">intercept =</span> <span class="ot">TRUE</span>, 
    <span class="dt">kernel =</span> <span class="ot">FALSE</span>) {
    
    <span class="co"># if p &gt; n, linear kernel ridge regression</span>
    if ((p &gt;<span class="st"> </span>n) |<span class="st"> </span>(kernel ==<span class="st"> </span><span class="ot">TRUE</span>)) {
        
        <span class="co"># SVD</span>
        svd =<span class="st"> </span><span class="kw">svd</span>(X.)
        
        <span class="co"># adjust d vector for regularization and diagonalize</span>
        d_adj =<span class="st"> </span><span class="kw">diag</span>(<span class="dv">1</span>/(svd$d^<span class="dv">2</span> +<span class="st"> </span>lam))
        
        <span class="co"># calculate beta estimates</span>
        betas =<span class="st"> </span><span class="kw">t</span>(X.) %*%<span class="st"> </span>svd$u %*%<span class="st"> </span>d_adj %*%<span class="st"> </span><span class="kw">t</span>(svd$u) %*%<span class="st"> </span>
<span class="st">            </span>y.
        <span class="kw">rownames</span>(betas) =<span class="st"> </span><span class="ot">NULL</span>
        
    } else {
        <span class="co"># compute normal ridge regression</span>
        
        <span class="co"># SVD</span>
        svd =<span class="st"> </span><span class="kw">svd</span>(X.)
        
        <span class="co"># adjust d vector for regularization and diagonalize</span>
        d_adj =<span class="st"> </span><span class="kw">diag</span>(svd$d/(svd$d^<span class="dv">2</span> +<span class="st"> </span>lam))
        
        <span class="co"># calculate beta estimates</span>
        betas =<span class="st"> </span>svd$v %*%<span class="st"> </span>d_adj %*%<span class="st"> </span><span class="kw">t</span>(svd$u) %*%<span class="st"> </span>y.
        <span class="kw">rownames</span>(betas) =<span class="st"> </span><span class="ot">NULL</span>
        
    }
    
    returns =<span class="st"> </span><span class="kw">list</span>(<span class="dt">coefficients =</span> betas)
    <span class="kw">return</span>(returns)
}</code></pre></div>

<p><strong>IRLS code snippet</strong>:</p>
<p>Note this is not the actual code. The real code is written in c++.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculates the coefficient estimates for logistic</span>
<span class="co"># regression (IRLS)</span>
IRLS =<span class="st"> </span>function(X, y, <span class="dt">lam =</span> <span class="dv">0</span>, <span class="dt">intercept =</span> <span class="ot">TRUE</span>, <span class="dt">tol =</span> <span class="dv">10</span>^(-<span class="dv">5</span>),
    <span class="dt">maxit =</span> <span class="fl">1e+05</span>, vec) {

    <span class="co"># IRLS algorithm</span>
    while ((iteration &lt;<span class="st"> </span>maxit) &amp;<span class="st"> </span>(<span class="kw">max</span>(<span class="kw">abs</span>(grads)) &gt;<span class="st"> </span>tol)) {

        <span class="co"># update working data</span>
        Xb =<span class="st"> </span>X %*%<span class="st"> </span>betas
        P =<span class="st"> </span><span class="kw">logitr</span>(Xb)
        weights =<span class="st"> </span><span class="kw">as.numeric</span>(P *<span class="st"> </span>(<span class="dv">1</span> -<span class="st"> </span>P))
        z =<span class="st"> </span>(y -<span class="st"> </span>P)/weights +<span class="st"> </span>Xb

        <span class="co"># calculate new betas</span>
        betas =<span class="st"> </span><span class="kw">linearr</span>(<span class="dt">X =</span> X, <span class="dt">y =</span> z, <span class="dt">lam =</span> <span class="fl">0.1</span>, <span class="dt">weights =</span> weights,
            <span class="dt">intercept =</span> intercept, <span class="dt">kernel =</span> <span class="ot">FALSE</span>)$coefficients

        <span class="co"># calculate updated gradients</span>
        grads =<span class="st"> </span><span class="kw">gradient_IRLS_logistic</span>(betas, X, y, lam,
            vec)
        iteration =<span class="st"> </span>iteration +<span class="st"> </span><span class="dv">1</span>
    }

    returns =<span class="st"> </span><span class="kw">list</span>(<span class="dt">coefficients =</span> betas, <span class="dt">total.iterations =</span> iteration, <span class="dt">gradient =</span> grads)
    <span class="kw">return</span>(returns)
}</code></pre></div>

</div>
<div id="majorize-minimization-mm" class="section level2">
<h2>Majorize-Minimization (MM)</h2>
<p>The second algorithm we propose to compute <span class="math inline">\(\hat{\beta}\)</span> is called majorize-minimization (MM). Unlike the IRLS algorithm, the MM can be used with the ridge penalty as well as the bridge penalty. The main idea is the following:</p>

<p>Similar to IRLS, the MM algorithm will be looped for <span class="math inline">\(k = 0, 1, ..., K\)</span> until convergence. Unlike IRLS, we will assume an intercept for this problem. We will show how to find <span class="math inline">\(Q\)</span> for penalized logistic regression. Let’s state our objective:</p>
<p><span class="math display">\[ \hat{\beta} = \arg\min_{\beta} -l(\beta) + \lambda\left(\gamma\frac{1}{2}\sum_{j = 1}^{p}\beta_{j}^{2} +(1 - \gamma) \frac{1}{\alpha}\sum_{j = 1}^{p}\left|\beta_{j} \right|^{\alpha}\right)  \]</span></p>
<p>where <span class="math inline">\(\alpha \in \left(1, 2 \right)\)</span> and <span class="math inline">\(\gamma \in \left\{0, 1 \right\}\)</span> will act as indicator function. If <code>penalty = &quot;ridge&quot;</code> then <span class="math inline">\(\gamma = 1\)</span>. If <code>penalty = &quot;bridge&quot;</code> then <span class="math inline">\(\gamma = 0\)</span> and the appropriate terms will be set to zero.</p>
</div>
<div id="solve-for-qbeta-betak" class="section level2">
<h2>Solve for <span class="math inline">\(Q(\beta | \beta^{(k)})\)</span></h2>
<p>Note the following fact: if for some function <span class="math inline">\(\phi\)</span>, <span class="math inline">\(\phi''(x) &lt; 0\)</span> for all <span class="math inline">\(x\)</span> then</p>
<p><span class="math display">\[ \phi(x) \leq \phi(x^{(k)}) + \phi'(x^{(k)})(x - x^{(k)}) \]</span></p>
<p>Let <span class="math inline">\(\phi(x) = (x)^{\alpha/2}\)</span>:</p>
<p><span class="math display">\[\frac{d}{dx}(x^{\alpha/2}) = \frac{\alpha}{2}x^{\alpha/2-1} &gt; 0 \]</span></p>
<p><span class="math display">\[\frac{d^{2}}{d^{2}x}(x^{\alpha/2}) = \frac{\alpha}{2}(\frac{\alpha}{2} - 1)x^{\alpha/2-2} &lt; 0 \]</span></p>
<p>This implies that the following inequality must hold:</p>
<p><span class="math display">\[ \left|\beta_{j}\right|^{\alpha} = (\beta_{j}^{2})^{\alpha/2} \leq (\beta_{j}^{(k)^{2}})^{\alpha/2} + \frac{\alpha}{2}(\beta_{j}^{(k)^{2}})^{\alpha/2 - 1}(\beta_{j}^{2} - \beta_{j}^{(k)^{2}}) \]</span></p>

<p>We now derive our <span class="math inline">\(Q\)</span> function:</p>
<span class="math display">\[\begin{align}
-l(\beta) &amp;+ \lambda\left(\gamma\frac{1}{2}\sum_{j = 1}^{p}\beta_{j}^{2} +(1 - \gamma) \frac{1}{\alpha}\sum_{j = 1}^{p}\left|\beta_{j} \right|^{\alpha}\right) \\
  &amp;=  -l(\beta^{(k)}) + \frac{1}{2}(\beta - \beta^{(k)})^{T}(\nabla^{2} -l(\beta^{(k)}))(\beta - \beta^{(k)}) + (\nabla - l(\beta^{(k)}))^{T}(\beta - \beta^{(k)}) \\
  &amp;+ \lambda\left(\gamma\frac{1}{2}\sum_{j = 1}^{p}\beta_{j}^{2} +(1 - \gamma) \frac{1}{\alpha}\sum_{j = 1}^{p}\left|\beta_{j} \right|^{\alpha}\right) \\
  &amp;\leq \frac{1}{2}(\beta - \beta^{(k)})^{T}(\nabla^{2} -l(\beta^{(k)}))(\beta - \beta^{(k)}) + (\nabla - l(\beta^{(k)}))^{T}(\beta - \beta^{(k)}) \\
  &amp;+ \lambda\left(\gamma\frac{1}{2}\sum_{j = 1}^{p}\beta_{j}^{2} +(1 - \gamma) \frac{1}{\alpha}\sum_{j = 1}^{p}\left(\beta_{j}^{(k)^{2}})^{\alpha/2} + \frac{\alpha}{2}(\beta_{j}^{(k)^{2}})^{\alpha/2 - 1}(\beta_{j}^{2} - \beta_{j}^{(k)^{2}} \right)\right) + const. \\
  &amp;\leq \frac{1}{2}(\beta - \beta^{(k)})^{T}(\nabla^{2} -l(\beta^{(k)}))(\beta - \beta^{(k)}) + (\nabla - l(\beta^{(k)}))^{T}(\beta - \beta^{(k)}) \\
  &amp;+ \lambda\left(\frac{\gamma}{2}(v \circ \beta)^{T}\beta +(1 - \gamma) \frac{1}{2}\sum_{j = 1}^{p}(\beta_{j}^{(k)^{2}})^{\alpha/2 - 1}\beta_{j}^{2} \right) + const. \\
  &amp;\leq \frac{1}{2}(\beta - \beta^{(k)})^{T}(\nabla^{2} -l(\beta^{(k)}))(\beta - \beta^{(k)}) + (\nabla - l(\beta^{(k)}))^{T}(\beta - \beta^{(k)}) \\
  &amp;+ \lambda\left(\frac{\gamma}{2}(v \circ \beta)^{T}\beta +(1 - \gamma) \frac{1}{2}\beta^{T}diag(d^{(k)})\beta \right) + const. \\
  &amp; \nonumber \\
  &amp;\mbox{ where $v \circ \beta$ is the dot product between $\beta$ and a $(p + 1) \times 1$ vector $v$ with all entries equal to} \nonumber \\
  &amp;\mbox{ zero except the first ($v = (0, 1, 1, 1,...)^{T}$). $d^{(k)}$ is the column vector with $d_{1}^{(k)} = 0$ and} \nonumber \\
  &amp;\mbox{ $d_{j}^{(k)} = (\beta_{j}^{(k)^{2}})^{\alpha/2 -1}$ for $j = 2,..,p$ and $\delta = 10^{-5}$ (some small number)} \nonumber \\
  &amp; \nonumber \\
  &amp;= \frac{1}{2}(\beta - \beta^{(k)})^{T}(\nabla^{2} -l(\beta^{(k)}))(\beta - \beta^{(k)}) + (\nabla - l(\beta^{(k)}))^{T}(\beta - \beta^{(k)}) \\
  &amp;+ \frac{\lambda}{2}\left(\beta^{T} diag\left[\gamma(v - d^{(k)}) + d^{(k)} \right]\beta \right) + const. \\
  &amp;\leq \frac{1}{2}(\beta - \beta^{(k)})^{T}X^{T}(\frac{1}{4} + \delta)X(\beta - \beta^{(k)}) + (\nabla - l(\beta^{(k)}))^{T}(\beta - \beta^{(k)}) \\
  &amp;+ \frac{\lambda}{2}\left(\beta^{T} diag\left[\gamma(v - d^{(k)}) + d^{(k)} \right]\beta \right) + const. \\
  &amp;=: Q(\beta | \beta^{(k)})
\end{align}\]</span>
<p>That was a lot of effort to find our <span class="math inline">\(Q\)</span> function but we eventually see that our <span class="math inline">\(Q\)</span> function is quadratic in form. Therefore, we can find a closed solution for each update. By plugging in the gradient and hessian of our log-likelihood function and dropping constants, we will eventually see by solving for the gradient of <span class="math inline">\(Q\)</span> and setting it equal to zero, that we have:</p>
<p><span class="math display">\[\beta^{(k + 1)} = \left(X^{T}(\frac{1}{4} + \delta)X +\lambda diag \left[\gamma(v - d^{(k)}) + d^{(k)}\right] \right)^{-1}\left(X^{T}(y - p^{(k)}) + X^{T}(\frac{1}{4} + \delta)X \right) \]</span></p>
<p>Finally, putting everything together we have the MM algorithm for penalized logistic regression:</p>

<p><strong>Algorithm</strong>:</p>
<ul>
<li>Initialize <span class="math inline">\(\beta^{(0)}\)</span>.</li>
<li>For <span class="math inline">\(k = 0, ..., K\)</span> until convergence:
<ul>
<li><span class="math inline">\(\beta^{(k + 1)} = \left(X^{T}(\frac{1}{4} + \delta)X +\lambda diag \left[\gamma(v - d^{(k)}) + d^{(k)}\right] \right)^{-1}\left(X^{T}(y - p^{(k)}) + X^{T}(\frac{1}{4} + \delta)X \right)\)</span></li>
</ul></li>
</ul>

<p>Below you can find a snippet of code for the MM function.</p>

<p><strong>MM code snippet</strong>:</p>
<p>Note this is not the actual code. The real code is written in c++.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># calculates the coefficient estimates for logistic</span>
<span class="co"># regression (MM)</span>
MM =<span class="st"> </span>function(X, y, <span class="dt">lam =</span> <span class="dv">0</span>, <span class="dt">alpha =</span> <span class="fl">1.5</span>, <span class="dt">gamma =</span> <span class="dv">1</span>, <span class="dt">intercept =</span> <span class="ot">TRUE</span>, 
    <span class="dt">tol =</span> <span class="dv">10</span>^(-<span class="dv">5</span>), <span class="dt">maxit =</span> <span class="fl">1e+05</span>, <span class="dt">vec =</span> <span class="ot">NULL</span>) {
    
    <span class="co"># MM algorithm</span>
    while ((iteration &lt;<span class="st"> </span>maxit) &amp;<span class="st"> </span>(<span class="kw">max</span>(<span class="kw">abs</span>(grads)) &gt;<span class="st"> </span>tol)) {
        
        <span class="co"># update d vector</span>
        d =<span class="st"> </span><span class="kw">as.numeric</span>((betas^<span class="dv">2</span>)^(alpha/<span class="dv">2</span> -<span class="st"> </span><span class="dv">1</span>))
        d[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">0</span>
        
        <span class="co"># qrsolve</span>
        betas =<span class="st"> </span><span class="kw">qr.solve</span>(Z +<span class="st"> </span>lam *<span class="st"> </span><span class="kw">diag</span>(gamma *<span class="st"> </span>(vec -<span class="st"> </span>d) +<span class="st"> </span>
<span class="st">            </span>d), <span class="kw">t</span>(X) %*%<span class="st"> </span>(y -<span class="st"> </span><span class="kw">logitr</span>(X %*%<span class="st"> </span>betas)) +<span class="st"> </span>Z %*%<span class="st"> </span>
<span class="st">            </span>betas)
        <span class="kw">rownames</span>(betas) =<span class="st"> </span><span class="ot">NULL</span>
        
        <span class="co"># calculate updated gradients</span>
        grads =<span class="st"> </span><span class="kw">gradient_MM_logistic</span>(betas, X, y, lam, alpha, 
            gamma, vec)
        iteration =<span class="st"> </span>iteration +<span class="st"> </span><span class="dv">1</span>
        
    }
    
    returns =<span class="st"> </span><span class="kw">list</span>(<span class="dt">coefficients =</span> betas, <span class="dt">total.iterations =</span> iteration, 
        <span class="dt">gradient =</span> grads)
    <span class="kw">return</span>(returns)
}</code></pre></div>

</div>
<div id="usage" class="section level2">
<h2>Usage</h2>
<p>Let us walk through some functions in the <code>logitr</code> package. We will use the iris data set for illustration.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(logitr)

<span class="co">#we will use the iris data set</span>
X =<span class="st"> </span>dplyr::<span class="kw">select</span>(iris, -<span class="kw">c</span>(Species, Sepal.Length))
y =<span class="st"> </span>dplyr::<span class="kw">select</span>(iris, Sepal.Length)
y_class =<span class="st"> </span><span class="kw">ifelse</span>(dplyr::<span class="kw">select</span>(iris, Species) ==<span class="st"> &quot;setosa&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>)

<span class="co">#ridge regression</span>
<span class="kw">linearr</span>(X, y, <span class="dt">lam =</span> <span class="fl">0.1</span>)</code></pre></div>
<pre><code>## $coefficients
##           Sepal.Length
## intercept    1.8778524
##              0.6462400
##              0.7023063
##             -0.5415988</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#ridge logistic regression (IRLS)</span>
<span class="kw">logisticr</span>(X, y_class, <span class="dt">lam =</span> <span class="fl">0.1</span>, <span class="dt">penalty =</span> <span class="st">&quot;ridge&quot;</span>)</code></pre></div>
<pre><code>## $coefficients
##                [,1]
## intercept  6.276283
##            1.540809
##           -3.641782
##           -1.630507
## 
## $MSE
## [1] 5.13471e-05
## 
## $log.loss
## [1] 0.3956525
## 
## $misclassification
## [1] 0
## 
## $total.iterations
## [1] 11
## 
## $gradient
##                   [,1]
## intercept 4.536561e-11
##           1.175446e-10
##           1.671454e-10
##           5.304043e-11</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#ridge logistic regression (MM)</span>
<span class="kw">logisticr</span>(X, y_class, <span class="dt">lam =</span> <span class="fl">0.1</span>, <span class="dt">penalty =</span> <span class="st">&quot;ridge&quot;</span>, <span class="dt">method =</span> <span class="st">&quot;MM&quot;</span>)</code></pre></div>
<pre><code>## $coefficients
##                [,1]
## intercept  6.276226
##            1.540818
##           -3.641769
##           -1.630502
## 
## $MSE
## [1] 5.134801e-05
## 
## $log.loss
## [1] 0.3956563
## 
## $misclassification
## [1] 0
## 
## $total.iterations
## [1] 5459
## 
## $gradient
##                   [,1]
## intercept 1.831808e-06
##           5.390560e-06
##           9.993386e-06
##           3.516598e-06</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#bridge logistic regression (MM)</span>
fit =<span class="st"> </span><span class="kw">logisticr</span>(X, y_class, <span class="dt">lam =</span> <span class="fl">0.1</span>, <span class="dt">alpha =</span> <span class="fl">1.2</span>, <span class="dt">penalty =</span> <span class="st">&quot;bridge&quot;</span>)</code></pre></div>
<pre><code>## [1] &quot;using MM algorithm...&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit</code></pre></div>
<pre><code>## $coefficients
##                 [,1]
## intercept 13.0803079
##            0.6170122
##           -5.7168400
##           -0.2300421
## 
## $MSE
## [1] 2.749702e-05
## 
## $log.loss
## [1] 0.1878654
## 
## $misclassification
## [1] 0
## 
## $total.iterations
## [1] 26021
## 
## $gradient
##                   [,1]
## intercept 1.790042e-06
##           5.268013e-06
##           9.997899e-06
##           3.518574e-06</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#predict using bridge logistic regression estimates</span>
<span class="kw">predict_logisticr</span>(fit, X[<span class="dv">1</span>:<span class="dv">3</span>,], y_class[<span class="dv">1</span>:<span class="dv">3</span>])</code></pre></div>
<pre><code>## $fitted.values
##           [,1]
## [1,] 0.9992467
## [2,] 0.9989747
## [3,] 0.9994881
## 
## $class
##      [,1]
## [1,]    1
## [2,]    1
## [3,]    1
## 
## $MSE
## [1] 6.269174e-07
## 
## $log.loss
## [1] 0.002291457
## 
## $misclassification
## [1] 0</code></pre>

</div>
<div id="example" class="section level2">
<h2>Example</h2>
<p>Let us now consider a real-life data set. We will use the High Time Resolution Universe Survey (HTRU) data set. The goal of this study was to classify Pulsar (type of star) candidates. From the website: “Pulsars are a rare type of Neutron star that produce radio emission detectable here on Earth. They are of considerable scientific interest as probes of space-time, the inter-stellar medium, and states of matter” (1). This data set has 17,898 rows and 9 columns – what each column means is not of interest to us (we are only interested in computation time).</p>

<p>Link : <a href="https://archive.ics.uci.edu/ml/datasets/HTRU2">HTRU2 Data</a> </p>
<p>In the following code, we see that is only takes 0.2 seconds to compute our estimates using IRLS method and only 3.6 seconds to compute them using the MM algorithm.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data =<span class="st"> </span><span class="kw">read.csv</span>(<span class="st">&quot;/Users/Matt/Documents/Grad School/Second Year/STAT 8054/HTRU_2.csv&quot;</span>, <span class="dt">header =</span> F)

<span class="kw">dim</span>(data)</code></pre></div>
<pre><code>## [1] 17898     9</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(data)</code></pre></div>
<pre><code>##          V1       V2          V3         V4       V5       V6        V7
## 1 140.56250 55.68378 -0.23457141 -0.6996484 3.199833 19.11043  7.975532
## 2 102.50781 58.88243  0.46531815 -0.5150879 1.677258 14.86015 10.576487
## 3 103.01562 39.34165  0.32332837  1.0511644 3.121237 21.74467  7.735822
## 4 136.75000 57.17845 -0.06841464 -0.6362384 3.642977 20.95928  6.896499
## 5  88.72656 40.67223  0.60086608  1.1234917 1.178930 11.46872 14.269573
## 6  93.57031 46.69811  0.53190485  0.4167211 1.636288 14.54507 10.621748
##          V8 V9
## 1  74.24222  0
## 2 127.39358  0
## 3  63.17191  0
## 4  53.59366  0
## 5 252.56731  0
## 6 131.39400  0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">X =<span class="st"> </span>dplyr::<span class="kw">select</span>(data, -V9)
y =<span class="st"> </span>dplyr::<span class="kw">select</span>(data, V9)

<span class="co">#time IRLS</span>
<span class="kw">timeit</span>(<span class="kw">logisticr</span>(X, y, <span class="dt">lam =</span> <span class="fl">0.1</span>, <span class="dt">penalty =</span> <span class="st">&quot;ridge&quot;</span>))</code></pre></div>
<pre><code>##    user  system elapsed 
##   0.038   0.006   0.041</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#time MM</span>
<span class="kw">timeit</span>(<span class="kw">logisticr</span>(X, y, <span class="dt">lam =</span> <span class="fl">0.1</span>, <span class="dt">penalty =</span> <span class="st">&quot;ridge&quot;</span>, <span class="dt">method =</span> <span class="st">&quot;MM&quot;</span>))</code></pre></div>
<pre><code>##    user  system elapsed 
##   0.943   0.038   0.964</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#ridge regression using IRLS</span>
fit =<span class="st"> </span><span class="kw">logisticr</span>(X, y, <span class="dt">lam =</span> <span class="fl">0.1</span>, <span class="dt">penalty =</span> <span class="st">&quot;ridge&quot;</span>)
fit</code></pre></div>
<pre><code>## $coefficients
##                   [,1]
## intercept -8.918799296
##            0.029373816
##           -0.034879266
##            6.517871438
##           -0.609570710
##           -0.028534630
##            0.053154195
##            0.046456589
##           -0.004702337
## 
## $MSE
## [1] 0.01715031
## 
## $log.loss
## [1] 1307.936
## 
## $misclassification
## [1] 0.02056096
## 
## $total.iterations
## [1] 9
## 
## $gradient
##                   [,1]
## intercept 1.904996e-08
##           2.105113e-06
##           8.971560e-07
##           5.416822e-09
##           2.332257e-10
##           6.758361e-08
##           2.745393e-07
##           2.794070e-07
##           5.458546e-06</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#predictions</span>
<span class="kw">predict_logisticr</span>(fit, X[<span class="dv">1</span>:<span class="dv">3</span>,], y$V9[<span class="dv">1</span>:<span class="dv">3</span>])</code></pre></div>
<pre><code>## $fitted.values
##             [,1]
## [1,] 0.001018225
## [2,] 0.018336697
## [3,] 0.009291594
## 
## $class
##      [,1]
## [1,]    0
## [2,]    0
## [3,]    0
## 
## $MSE
## [1] 0.0001412017
## 
## $log.loss
## [1] 0.02886067
## 
## $misclassification
## [1] 0</code></pre>

</div>
<div id="computer-specs" class="section level2">
<h2>Computer Specs:</h2>
<ul>
<li>MacBook Pro (Late 2016)</li>
<li>Processor: 2.9 GHz Intel Core i5</li>
<li>Memory: 8GB 2133 MHz</li>
<li>Graphics: Intel Iris Graphics 550</li>
</ul>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<ol style="list-style-type: decimal">
<li><p>R. J. Lyon, B. W. Stappers, S. Cooper, J. M. Brooke, J. D. Knowles, Fifty Years of Pulsar Candidate Selection: From simple filters to a new principled real-time classification approach, Monthly Notices of the Royal Astronomical Society 459 (1), 1104-1123, DOI: 10.1093/mnras/stw656</p></li>
<li><p>R. J. Lyon, HTRU2, DOI: 10.6084/m9.figshare.3080389.v1.</p></li>
</ol>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
